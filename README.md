# Smart Vision Captioner Agent (Frontend Edition)

A local-first web interface for generating captions with Salesforce BLIP and turning them into concise insights with an OpenRouter-hosted LLM. Everything runs in the browser: provide your own API keys, choose an image, and get actionable analysis in seconds.

## Features
- **Glassmorphic single-page UI** with responsive layout for desktop and mobile.
- **Image sources**: upload your own file or use optional samples stored in `smart-vision-captioner/samples/`.
- **Dual inference pipeline**:
  - Hugging Face Inference API (Salesforce BLIP family) for caption generation.
  - OpenRouter chat completion endpoint for refining captions into professional insights.
- **User-provided credentials only** – keys are saved to `localStorage` and never leave the browser.
- **Status feedback & error handling** to help you diagnose API or asset issues quickly.

## Repository Layout
```
smart-vision-captioner/
├── index.html      # Markup and layout
├── styles.css      # Dark, glassmorphic styling
├── script.js       # Front-end logic and API requests
└── samples/        # Optional user-supplied JPG images
```

## Prerequisites
1. A modern browser (Chrome, Edge, Firefox, or Safari) with JavaScript enabled.
2. API credentials:
   - **Hugging Face Token** with access to the Salesforce BLIP models.
   - **OpenRouter API Key** (e.g., to use `qwen/qwen2.5-7b-instruct:free`).
3. (Optional) Sample JPGs placed in `smart-vision-captioner/samples/` named:
   - `burnt_pcb.jpg`
   - `lab_equipment.jpg`
   - `classroom.jpg`

If the sample files are missing, the UI shows placeholders and explains how to add them.

## Getting Started
1. Clone or download this repository.
2. Open `smart-vision-captioner/index.html` directly in your browser (no build step required).
3. Enter your Hugging Face and OpenRouter keys in the **API Access** panel.
4. Pick an image (upload or click a sample button) and press **“Caption & Analyze.”**
5. Review the caption produced by BLIP and the insight generated by the OpenRouter model.

## Usage Notes
- The default prompt encourages short, factual insights with one recommendation. Customize it any time.
- You can switch BLIP or OpenRouter models via the dropdowns without reloading the page.
- Keys persist in the browser between sessions. Use the **Clear** button to reset the interface (keys remain until you manually remove them).
- All network requests go directly from your browser to Hugging Face and OpenRouter; no intermediaries are involved.

## Troubleshooting
| Issue | What to Check |
|-------|---------------|
| Sample button shows a warning | Ensure the corresponding JPG exists in `smart-vision-captioner/samples/` and refresh the page. |
| Hugging Face errors | Confirm your token is valid and has access to the selected BLIP model. Some models may take time to warm up. |
| OpenRouter errors | Verify that your key is active and that the selected model is available on the free tier. |
| Browser blocked requests | Check the developer console for CORS or network errors. Both APIs must be reachable from your network. |

## Security & Privacy
- Keys are written to `localStorage` so they persist locally; clear browser storage if you want to remove them.
- No analytics, telemetry, or backend services are included.

## License
This project is provided as-is for educational purposes. Review the terms of the Hugging Face and OpenRouter services before deploying publicly.
